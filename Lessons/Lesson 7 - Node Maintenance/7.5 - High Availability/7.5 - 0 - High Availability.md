- [Introduction](#introduction)
- [High Availability](#high-availability)
  - [Environment setup](#environment-setup)
  - [First Cluster](#first-cluster)
  - [HA - High Availability cluster](#ha---high-availability-cluster)

## Introduction

This readme tends to guide the path towards achieving a milestone: Run a High Availability Kubernetes Cluster locally. Using few resources and getting in touch with the underlying infrastructre. This is rare since most of the times, in production environments, the Cloud-Provider load this type of burden. Enjoy.

## High Availability

In order to meet the minium requirements according to [Kubernetes documentation](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/):

```bash
"Three or more machines that meet kubeadm's minimum requirements for the control-plane nodes. Having an odd number of control plane nodes can help with leader selection in the case of machine or zone failure"
```

I'll make a bet and try using [Linux Talos](https://www.talos.dev/), a Linux designed for Kubernetes.

Therefore I hope to be able to meet the HA requirements easier by setting the nodes at a virtualized using VirtualBox VM machines.

To make my way to HA smoother, I'll set two steps. First will mean creating a K8s cluster with no defined topology using the new toolset, and the second will mean creating a pre-defined HA topology. There we go.


-------------------------------
### Environment setup

Since I'm working in a Virtualized invornment, running on Oracle VirtualBox, there exist many configurations that worth mentioning. Should this not be your case, just skip this.

First step will involve creating the VM in VirtualBox, by clicking "New".

<img src="./Misc/1.PNG" width="20%">

Select the Talos ISO and set the Type/Subtype/Version:

<img src="./Misc/2.PNG" width="40%">

Set the hardware to meet the [minimum requirements](https://www.talos.dev/v1.9/introduction/system-requirements/):

<img src="./Misc/3.PNG" width="40%">

<img src="./Misc/3b.PNG" width="40%">

Before starting the node, change the network configuration:

<img src="./Misc/4.PNG" width="50%">


-------------------------------

### First Cluster

At this first approach, I'll run a Kubernetes Cluster with some Nodes in order to get familiar with the new working environmenmt. I'll not deploy any particular topology, just couple of working nodes and a control-plane.

To achive this, first create three Talos Nodes:

<img src="./Misc/5.PNG" width="15%">

After starting the nodes, both will look like the following:

<img src="./Misc/6.PNG" width="85%">

Nodes are waiting to get some configuration. Since this OS have a Read-Only file-system, the only way to configure and interact with the nodes, it using the API.

There we go. Start by installing [checking this guide](https://www.talos.dev/v1.9/introduction/getting-started/#talosctl).

Set the IPs for the components. (Check the nodes IP at the GUI).

```bash
export CONTROL_PLANE_NODE=192.168.1.44
export WORKER_NODE_1=192.168.1.45
export WORKER_NODE_1=192.168.1.46
```


Generate the files that will be applied via API to the nodes:
```bash
talosctl gen config TinyCluster https://$CONTROL_PLANE_NODE:6443 --output-dir _out 
```

```bash
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ talosctl gen config TinyCluster https://$CONTROL_PLANE_NODE:6443 --output-dir _out
generating PKI and tokens
Created _out/controlplane.yaml
Created _out/worker.yaml
Created _out/talosconfig
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ ls
_out
```

Before applying the configuration to the Control Node, check the output of its GUI. It should be "quiet" with no new output flowing.

Apply the cfg to the control-node:
```bash
talosctl apply-config --insecure --nodes $CONTROL_PLANE_NODE --file _out/controlplane.yaml
```
It will prompt the installation into the node:

<img src="./Misc/7.PNG" width="55%">

Once the installation finishes:

<img src="./Misc/8.PNG" width="55%">

It is time now to configure the talosctl endpoint:

```bash
talosctl config endpoint $CONTROL_PLANE_NODE
talosctl config node $CONTROL_PLANE_NODE
```

Check it has been setup properly:
```bash
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ cat _out/talosconfig
context: TinyCluster
contexts:
    TinyCluster:
        endpoints:
            - 192.168.1.44
        nodes:
            - 192.168.1.44
```

Now export the talos config:
```bash
export TALOSCONFIG="_out/talosconfig"
```

Start the etcd process in the control node:

```bash
talosctl bootstrap --nodes $CONTROL_PLANE_NODE
```

<img src="./Misc/9.PNG" width="55%">


Test the current status by doing:

```bash
talosctl containers -k --nodes $CONTROL_PLANE_NODE
NODE           NAMESPACE   ID                                                                                          IMAGE                                             PID    STATUS
192.168.1.44   k8s.io      kube-system/kube-apiserver-talos-a0l-qtk                                                    registry.k8s.io/pause:3.10                        2364   SANDBOX_READY
192.168.1.44   k8s.io      └─ kube-system/kube-apiserver-talos-a0l-qtk:kube-apiserver:79abd7d76e39                     registry.k8s.io/kube-apiserver:v1.32.0            2491   CONTAINER_RUNNING
192.168.1.44   k8s.io      kube-system/kube-controller-manager-talos-a0l-qtk                                           registry.k8s.io/pause:3.10                        2395   SANDBOX_READY
192.168.1.44   k8s.io      └─ kube-system/kube-controller-manager-talos-a0l-qtk:kube-controller-manager:479a72393885   registry.k8s.io/kube-controller-manager:v1.32.0   2444   CONTAINER_RUNNING
192.168.1.44   k8s.io      kube-system/kube-scheduler-talos-a0l-qtk                                                    registry.k8s.io/pause:3.10                        2405   SANDBOX_READY
192.168.1.44   k8s.io      └─ kube-system/kube-scheduler-talos-a0l-qtk:kube-scheduler:a2eb6f239d89                     registry.k8s.io/kube-scheduler:v1.32.0 
```

It is now time to setup the worker nodes. Now again, check the status of the output logs before proceeding. It will start flowing once the config is applied:

```bash
export WORKER_IP_1=192.168.1.45
export WORKER_IP_2=192.168.1.46
```

```bash
talosctl apply-config --insecure --nodes $WORKER_IP_1 --file _out/worker.yaml
talosctl apply-config --insecure --nodes $WORKER_IP_2 --file _out/worker.yaml
```

<img src="./Misc/10.PNG" width="55%">


Check the machines # that is available at all nodes once the installation finishes.


Once all the installations are down, remove the iso from the booting point and reboot three nodes:

<img src="./Misc/11.PNG" width="55%">


Create the Kubeconfig so as to be able to manage the Cluster, based on the talosconfig:

```bash
talosctl kubeconfig . --talosconfig _out/talosconfig
```

```bash
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ talosctl kubeconfig . --talosconfig _out/talosconfig
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ ls
kubeconfig  _out
```

&nbsp;

<details> 
<summary>What if kubectl is not configured correctly?</summary>

```bash
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ kubectl get nodes
E0118 15:11:18.166894   32303 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://127.0.0.1:39419/api?timeout=32s\": dial tcp 127.0.0.1:39419: connect: connection refused"
E0118 15:11:18.169197   32303 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://127.0.0.1:39419/api?timeout=32s\": dial tcp 127.0.0.1:39419: connect: connection refused"
E0118 15:11:18.171618   32303 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://127.0.0.1:39419/api?timeout=32s\": dial tcp 127.0.0.1:39419: connect: connection refused"
E0118 15:11:18.174195   32303 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://127.0.0.1:39419/api?timeout=32s\": dial tcp 127.0.0.1:39419: connect: connection refused"
E0118 15:11:18.176539   32303 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://127.0.0.1:39419/api?timeout=32s\": dial tcp 127.0.0.1:39419: connect: connection refused"
The connection to the server 127.0.0.1:39419 was refused - did you specify the right host or port?
```
</details>

Configure kubectl:

```bash
export KUBECONFIG=./kubeconfig
```

Check the output:

```bash
alesb@LightUbuntu:~/Desktop/Tiny-Talos$ kubectl get nodes
NAME            STATUS   ROLES           AGE     VERSION
talos-08p-q0z   Ready    <none>          9m27s   v1.32.0
talos-a0l-qtk   Ready    control-plane   13m     v1.32.0
talos-s54-bn9   Ready    <none>          9m28s   v1.32.0
```

Should be the case, that the worker node didn't get its role automatically, set it up by doing:
```bash
kubectl label node talos-08p-q0z  node-role.kubernetes.io/worker=worker
kubectl label node talos-s54-bn9  node-role.kubernetes.io/worker=worker
```

```bash
NAME            STATUS   ROLES           AGE   VERSION
talos-08p-q0z   Ready    worker          20m   v1.32.0
talos-a0l-qtk   Ready    control-plane   23m   v1.32.0
talos-s54-bn9   Ready    worker          20m   v1.32.0
```

Now create a deployment:
```bash
kubectl create deployment tinytalos --image=nginx --replicas=4
```

There it is! A working Cluster :)

```bash
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE            NOMINATED NODE   READINESS GATES
tinytalos-54d7f47d68-2hg66   1/1     Running   0          17s   10.244.2.3   talos-08p-q0z   <none>           <none>
tinytalos-54d7f47d68-6scvs   1/1     Running   0          17s   10.244.1.2   talos-s54-bn9   <none>           <none>
tinytalos-54d7f47d68-dkmvk   1/1     Running   0          17s   10.244.2.2   talos-08p-q0z   <none>           <none>
tinytalos-54d7f47d68-kbjng   1/1     Running   0          17s   10.244.1.3   talos-s54-bn9   <none>           <none>
```

-----------------------------------------

### HA - High Availability cluster