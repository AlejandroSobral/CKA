## Restoring etcd

Before proceeding further with the restore, create a set of deployments to better observe the effects of the restore, if no deploy exist.

<details>
  <summary>Create deploy and do-backup.</summary>

Can take the following as example:

```bash
kubectl create deployment restore-2-example --image=nginx --replicas=3
kubectl create deployment restore-example --image=nginx --replicas=3
```

```bash
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
restore-2-example   3/3     3            3           7s
restore-example     3/3     3            3           29s
```

Perform the backup itself. This has been wide explain in Lesson 7.2
```bash
sudo ETCDCTL_API=3 etcdctl --endpoints=localhost:2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key snapshot save /tmp/etcdbackup.db
```
</details>

&nbsp;

Before starting, you can check the official [Documentation](https://etcd.io/docs/v3.5/op-guide/recovery/) if you like.


---------------------------------------


The following steps require the operator to read carefully the **whole instruction set before proceeding**. Otherwhise, *it can harm its deploy*.

Restoring the etcd content means using a new directory in a non-default location to store the *new* etcd ongoing. 
At the same time, The Kubernetes core services must be stopped before changing the location. After doing it, the etcd can be reconfigured to use the new directory.
&nbsp;

Stopping the Kubernetes Core Services means stopping it Statis Podes. As it has been previously review, Static Pods have its own yaml file at /etc/kubernetes/manifests. Therefore, we can get what it is running in Control Node by doing the following, and there should be a matching:

```bash
alesb@ControlNode:/$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```
&nbsp;

Therefore, if the Control-Node Static Pods are checked by doing:

```bash
sudo crictl ps
```
At least the following will appear, among many others:

```bash
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
...
..
.
0e3902f9dacff       2e96e5913fc06       30 minutes ago      Running             etcd                      19                  492628ee6d6e2       etcd-controlnode
dddcb9baec0de       6bab7719df100       30 minutes ago      Running             kube-apiserver            14                  62fbaef594500       kube-apiserver-controlnode
5b3c8d588a580       9aa1fad941575       30 minutes ago      Running             kube-scheduler            14                  1d885dc9ef847       kube-scheduler-controlnode
af61148b044a8       175ffd71cce3d       30 minutes ago      Running             kube-controller-manager   14                  658dd22871da9       kube-controller-manager-controlnode
.
..
...
```
&nbsp;



Should the core services YAMLs be removed from its folder, the Pods will be wiped-out and the Kubernetes cluster will be stopped and ready to proceed with the etcd Restore.

But before proceeding, do not forget to delete some deployments, to see if the restore really worked:
```bash
kubectl delete deploy restore-example
deployment.apps "restore-example" deleted
```
After the following step, you will not be able to even list the deployed resources within the cluster.

Stop core services:
```bash
sudo mkdir /etc/kubernetes/backup-yamls
cd /etc/kubernetes/manifests
sudo mv *.yaml /etc/kubernetes/backup-yamls
```

Check if the Static Pods disapppear. If not, wait some minutes or check the previous steps execution.
```bash
sudo crictl ps
```
The list should not include any of the listed before; etcd, kube-apiserver, kube-controller-manager, kube-scheduler.

&nbsp;

Now regarding the backup itself, the following command will restore the etcd content from /tmp/etcdbackup.db and set it into a new file directory, /var/lib/etcd-backup:

```bash
sudo ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcdbackup.db --data-dir /var/lib/etcd
```
Expected output will look like:
```bash
2024-12-27 13:49:27.245365 I | mvcc: restore compact to 168714
2024-12-27 13:49:27.299211 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
```

Check that a new folder has been created:

```bash
sudo ls -l /var/lib/etcd-backup/
```

Now edit the set that points to the etcd working directory at:

```bash
sudo nano /etc/kubernetes/backup-yamls/etcd.yaml
```


Look for spec.volumes.hostPath, with type: DirectoryOrCreate
```YAML
spec:
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
```

Change it for the new one:
```YAML
spec:
  volumes:
  - hostPath:
      path: /var/lib/etcd-backup
      type: DirectoryOrCreate
```

Now is time to restore the Static Pods files back:

```bash
sudo mv /etc/kubernetes/backup-yamls/*.yaml /etc/kubernetes/manifests
```

Check that the Static Pods are again in place:

```bash
sudo crictl ps
```

```bash
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d9cfa5c113457       45ae357729e3a       11 seconds ago      Running             calico-kube-controllers   23                  6bd35f31a9045       calico-kube-controllers-7f764f4f68-9hmpd
f5a2c8aec7545       175ffd71cce3d       38 seconds ago      Running             kube-controller-manager   0                   7731c38802f2a       kube-controller-manager-controlnode
1feea5a68559d       2e96e5913fc06       38 seconds ago      Running             etcd                      0                   8b7fad0dfd4a2       etcd-controlnode
44cbdb9677473       6bab7719df100       38 seconds ago      Running             kube-apiserver            0                   eb25031707b4d       kube-apiserver-controlnode
f705a312a0eb3       9aa1fad941575       38 seconds ago      Running             kube-scheduler            0                   684ced3efd929       kube-scheduler-controlnode
44893a23a51bb       c69fa2e9cbf5f       About an hour ago   Running             coredns                   13                  79347bbe183f9       coredns-7c65d6cfc9-dg86l
c6a60b5ceef50       c69fa2e9cbf5f       About an hour ago   Running             coredns                   13                  e6d86e4f47034       coredns-7c65d6cfc9-9zkf4
280b61eeb2175       44f52c09decec       About an hour ago   Running             calico-node               14                  2dded9d33e2c4       calico-node-8qppx
3207f73a1f263       60c005f310ff3       About an hour ago   Running             kube-proxy                14                  ac642ed4801ca       kube-proxy-tp4lv
```

&nbsp;

Now, considering that a deploy has been deleted, let's check that it has been succesfully restored:

```bash 
kubectl get deploy
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
restore-2-example   3/3     3            3           65m
restore-example     3/3     3            3           66m
```

Altough the deploy named **restore-example** has been deleted, is now in place again. Therefore, it worked perfect!

Is now up to you to set the path to reverse the whole thing!.



