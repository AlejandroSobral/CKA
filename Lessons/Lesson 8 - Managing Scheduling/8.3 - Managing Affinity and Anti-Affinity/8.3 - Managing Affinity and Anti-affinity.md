## 8.3 Managing Affinity and Anti-affinity.

- [8.3 Managing Affinity and Anti-affinity.](#83-managing-affinity-and-anti-affinity)
  - [Demo](#demo)
  - [Node Affinity](#node-affinity)
  - [Pod AntiAffinity](#pod-antiaffinity)
  - [Pod Affinity](#pod-affinity)


My humble advise for this will simply be: 

<details>
<summary><b>Check the following Docs.</b></summary>

&nbsp; 

https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity

------------------------------

</details>

&nbsp;

However, I'll explain it briefly.

Pods can be scheduled according to setups. Those setups allows to:
- To Schedule (or not to) Pods in certains Nodes, depending upon:
  - Node configuration/role/label
  - Pod Affinity: Schedule the Pod only in a Node where a particular sort of Pod have been already scheduled.
  - Pod PodAntiaffinity: If there exist any pod of a particular sort, do not schedule it in there. 

--------------------------------------------------------------------------------------

### Demo

To show this working, I'll set 3 worker nodes. Will be named as the following:

- worker0: blue
- worker1: red
- worker2: green

The plan consists in:


1) Set Pods by NodeAffinity by color.
   1) There will exist 3 deploys, blue-red-green, an matching Node color using NodeAffinity
      1) Naming will be: single-pod-{color}
   2) An extra Pod will be deployed with color 'gray' and its behaviour will be analyzed.
   
2) Set Pods Anti-Affinity in order to have a single Pod per Node. 
   1) Only 1 deploy. Named: anti-affinity-deploy
   2) More than 3 Pods will be created.
   
3) Set Pod Affinity matching to have new Pods only were previously Pods were created.
   1) Only 1 deploy. Named: Pod-affinity-deploy
   
---------------------------------------------------------------------------------
### Node Affinity

My current deploy consist of 3 worker nodes:
```bash
NAME            STATUS   ROLES           AGE     VERSION
talos-9cm-iu6   Ready    control-plane   2d19h   v1.32.0
talos-1p6-m7a   Ready    worker          18h     v1.32.0 -- w0
talos-2pv-ytc   Ready    worker          30h     v1.32.0 -- w1
talos-pmv-2qi   Ready    worker          2d19h   v1.32.0 -- w2
```

Label the worker nodes:
```bash
kubectl label node talos-1p6-m7a  color=blue
kubectl label node talos-2pv-ytc  color=red
kubectl label node talos-pmv-2qi  color=green
```


Verify the labeling:

```bash
kubectl get nodes -l color=red
NAME            STATUS   ROLES    AGE   VERSION
talos-2pv-ytc   Ready    worker   31h   v1.32.0

kubectl get nodes -l color=green
NAME            STATUS   ROLES    AGE     VERSION
talos-pmv-2qi   Ready    worker   2d19h   v1.32.0

kubectl get nodes -l color=blue
NAME            STATUS   ROLES    AGE   VERSION
talos-1p6-m7a   Ready    worker   18h   v1.32.0
```

Apply the Pod manifests for each color:
```bash
kubectl apply -f single-pod-red.yaml
pod/red-pod-deploy created

kubectl apply -f single-pod-green.yaml
pod/green-pod-deploy created

kubectl apply -f single-pod-gray.yaml
pod/gray-pod-deploy created
```

The result is the expected one:
```bash
kubectl get pods -o wide

NAME               READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES
gray-pod-deploy    0/1     Pending   0          21s     <none>        <none>          <none>           <none>
green-pod-deploy   1/1     Running   0          2m6s    10.244.0.32   talos-pmv-2qi   <none>           <none>
red-pod-deploy     1/1     Running   0          6m10s   10.244.4.4    talos-2pv-ytc   <none>           <none>
```

Red and green have been scheduled correctly. 

Once the scheduling confirmed as the expected, let's analize the -gray- case:

```bash
kubectl describe pod/gray-pod-deploy

Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  95s   default-scheduler  0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.
```

As no node match the 'color' affinity selector, it can't be deployed into any node.


-------------------------------------
### Pod AntiAffinity

Now moving to the second deploy. For this case, if there exist a Pod already scheduled into the Node, a second Pod of the same deployment is forbidden to be scheduled.

The replica count was 5, while the worker nodes are 3. Therefore the result is the expected:

```bash
kubectl apply -f single-pod-per-node.yaml
pod/single-pod-per-node created
```

```bash
NAME                                   READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES
single-pod-per-node-797f44b486-95jdt   1/1     Running   0          46s   10.244.0.36   talos-pmv-2qi   <none>           <none>
single-pod-per-node-797f44b486-j4xwj   1/1     Running   0          46s   10.244.1.5    talos-1p6-m7a   <none>           <none>
single-pod-per-node-797f44b486-vgg4v   1/1     Running   0          46s   10.244.4.7    talos-2pv-ytc   <none>           <none>
single-pod-per-node-797f44b486-pxc5b   0/1     Pending   0          46s   <none>        <none>          <none>           <none>
single-pod-per-node-797f44b486-9wm8w   0/1     Pending   0          46s   <none>        <none>          <none>           <none>
```

When describing the -Pending- pods; I've found:

```bash
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2m4s  default-scheduler  0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match pod anti-affinity rules. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod.
```

The PodAffinity has been proved correctly.

----------------------------------------
### Pod Affinity

Now I'll scale the deployment down to 2 replicas, to make it easier for the further steps and show better the effects of the third part.

```bash
kubectl scale deployment single-pod-per-node --replicas=2
deployment.apps/single-pod-per-node scaled

kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES
single-pod-per-node-797f44b486-95jdt   1/1     Running   0          3m58s   10.244.0.36   talos-pmv-2qi   <none>           <none>
single-pod-per-node-797f44b486-j4xwj   1/1     Running   0          3m58s   10.244.1.5    talos-1p6-m7a   <none>           <none>
```

Now a deployment will be created that will schedule pods only where the previous one were already deployed.


As expected, the "affinity-deploy" Pods get scheduled only in the pod where already existed a Pod from the previous deploy:

```bash
kubectl apply -f single-pod-affinity.yaml
pod/affinity-deploy created
```

&nbsp;

```bash
NAME                                   READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES
affinity-deploy-649fb48b7c-lmb8j       1/1     Running   0          46s   10.244.4.13   talos-2pv-ytc   <none>           <none>
affinity-deploy-649fb48b7c-ntn5z       1/1     Running   0          46s   10.244.4.14   talos-2pv-ytc   <none>           <none>
affinity-deploy-649fb48b7c-qkn42       1/1     Running   0          46s   10.244.1.7    talos-1p6-m7a   <none>           <none>
single-pod-per-node-797f44b486-hm8dl   1/1     Running   0          53s   10.244.4.12   talos-2pv-ytc   <none>           <none>
single-pod-per-node-797f44b486-tcrfs   1/1     Running   0          53s   10.244.1.6    talos-1p6-m7a   <none>           <none>
```