
## 9.2 Undestanding Service Auto registration

Kubernetes service autoregistration refers to how Kubernetes automatically registers services within the cluster's internal DNS when they are created.
Kubernetes runs the coredns Pods in the kube-system Namespace as internal DNS servers. These Pods are exposed by the [kubedns](https://kubernetes.io/docs/tasks/administer-cluster/coredns/) Service. Services register with this kubedns Service. 
Pods are automatically configured with the IP address of the [kubedns](https://kubernetes.io/docs/tasks/administer-cluster/coredns/) Service as their DNS resolver. Consequently, all Pods can access all Services by name. [kubedns is running Pods?](#kubedns-is-running-pods)

If a Service is running in the same Namespace, it can be reached by the short hostname.
If a Service is running in another Namespace, an FQDN consisting of servicename.namespace.svc.clustername must be used.

This will be demonstrated shortly.


Example:

Imagine you have a Service called my-web-service.  It has a label selector app=web.  You then deploy three Pods, each with the label app=web.  Kubernetes automatically registers these three Pods with the my-web-service.  Any application that tries to access my-web-service will be routed to one of these three Pods.  If you scale up to five Pods with the same label, the Service automatically updates to include the new Pods.  If one Pod crashes, it's removed from the Service.



---------------------------------------------------------------------

### kubedns is running Pods?

Answer is simple, yes:

```bash
kubectl get pods -n kube-system

NAME                                    READY   STATUS    RESTARTS         AGE
coredns-578d4f8ffc-fwsbn                1/1     Running   13 (2m2s ago)    23d
coredns-578d4f8ffc-km57d                1/1     Running   13 (2m2s ago)    23d
```
&nbsp;


----------------------------------------------------

#### Working inside a Namespace

By analzying the following example, I'll try to expose the nature of K8s auto-registration services:


Let's start by creating a single Pod, a tiny web server:
```bash
kubectl run tiny-web --image=nginx
pod/tiny-web created
```

&nbsp;

Let's now create the second, which is a tiny Linux that allow to run a few commands:

```bash
kubectl run test-pod --image=busybox -- sleep 7200
pod/test-pod created
```
&nbsp;

Try to reach out the tiny-web server using the 2nd Pod:
```bash
kubectl exec -it test-pod -- wget tiny-web
wget: bad address 'tiny-web'
```

Kubernetes DNS service can't solve "tiny-web" as an address.

What if we aim to its IP now?

```bash
kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES
test-pod   1/1     Running   0          3m38s   10.244.0.94   talos-pmv-2qi   <none>           <none>
tiny-web   1/1     Running   0          4m12s   10.244.0.93   talos-pmv-2qi   <none>           <none>
```

&nbsp;

```bash
kubectl exec -it test-pod -- wget 10.244.0.93
Connecting to 10.244.0.93 (10.244.0.93:80)
saving to 'index.html'
index.html           100% |********************************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved
```

As expected, it works perfectly.
&nbsp;

Therefore, it is easy to comprehend that there must be an extra layer in between for the BusyBox Pod to reach the tiny-web Pod.

```bash
kubectl expose pod tiny-web --port=80
service/tiny-web exposed

kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   24d
tiny-web     ClusterIP   10.101.121.235   <none>        80/TCP    7s
```
&nbsp;

As you might be foreseeing, this will now solve the DNS:

```bash
kubectl exec -it test-pod -- wget tiny-web
Connecting to tiny-web (10.101.121.235:80)
saving to 'index.html'
index.html           100% |********************************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved
```
Check that the Pod connected to the service IP instead of the Pod IP. Of course, the service then redirect the traffic using the labels.
To conclude; The selector on the Service matches the label on the Pod. Â  

&nbsp;
----------------------

### Working with different Namespaces

Create a 'remote' namespace:

```bash
kubectl create ns remote
namespace/remote created
```

Deploy a single pod:
```bash
kubectl run interginx --image=nginx
pod/interginx created
```

Deploy a single Pod in the "remote" namespace:
```bash
kubectl run remotebox --image=busybox -n remote -- sleep 3600
pod/remotebox created
```

Get the Pod IP:

```bash
kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES
interginx   1/1     Running   0          24s   10.244.0.95   talos-pmv-2qi   <none>           <none>
```

Try to reach the Pod using its IP:
```bash
kubectl exec -it -n remote remotebox -- wget 10.244.0.95
Connecting to 10.244.0.95 (10.244.0.95:80)
saving to 'index.html'
index.html           100% |********************************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved
```

&nbsp;

As you might have expected, it worked. But in a dynamic context, as it is in Kuberentes, how on earth shall the IPs be known?
Well, as you might imagine, that's when DNS comes into picture...same idea that in the outside-K8s world.

What if now we try using to expose the service and point at it by using its name as we did it recently?

```bash
kubectl expose pod interginx --port=80
service/interginx exposed
```
&nbsp;

Check the new-service existance:
```bash
kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
interginx    ClusterIP   10.99.76.55      <none>        80/TCP    5s
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   24d
```
&nbsp;

Once the new service if fully operational, it will allow to connect to the Pods:
```bash
kubectl exec -it -n remote remotebox -- wget interginx
wget: bad address 'interginx'
command terminated with exit code 1
```

What happened in here? Of course, since we tried to address the Pod from a different namespace, now its FQDN is required to reach it:


```bash
kubectl exec -it -n remote remotebox -- wget interginx.default.svc.cluster.local
Connecting to interginx.default.svc.cluster.local (10.99.76.55:80)
saving to 'index.html'
index.html           100% |********************************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved
```

There it is!