## Manage Nodes - Node State

With the following commands, nodes will set as unschedulable. Which is great for maintenance.

Mark a node a unschedulable.
```bash
kubectl cordon
``` 

Mark a node as unschedulable and remove all running Pods from it.
```bash
kubectl drain
``` 

Pods that have been started from a DaemonSet will not be removed while using kubectl drain, add --ignore-daemonsets to ignore that. This is because it is supposed to be this way with deamons.

For storage, if pods are using emptyDir Pod volumes, add --delete-emptydir-data to delete data from emptyDir Pod volumes. This are supposed to be ephemeral, anyway.

Get the node back in a schedulable state.
```bash
kubectl uncordon
``` 
---------------------------------------------
Now, to see it working, lets set a couple a pods running nginx images:

```bash
kubectl create deployment managenodes --image=nginx --replicas=10
```

To start checking this, start by doing:
```bash
kubectl get pods -o wide
``` 

Check the Pod distribution over the nodes. Some of the are running at WorkerNode, whereas other are running at WorkerNode2:
```bash
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS      AGE   IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-4q4fj   1/1     Running   0             11s   172.16.216.99    workernode2   <none>           <none>
managenodes-6b76565695-58q7x   1/1     Running   0             11s   172.16.190.210   workernode    <none>           <none>
managenodes-6b76565695-5vnjj   1/1     Running   0             11s   172.16.216.101   workernode2   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0             11s   172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-djfbp   1/1     Running   0             11s   172.16.216.103   workernode2   <none>           <none>
managenodes-6b76565695-g7n29   1/1     Running   0             11s   172.16.216.100   workernode2   <none>           <none>
managenodes-6b76565695-kw4p4   1/1     Running   0             11s   172.16.190.206   workernode    <none>           <none>
managenodes-6b76565695-nhhhl   1/1     Running   0             11s   172.16.216.102   workernode2   <none>           <none>
managenodes-6b76565695-r7djj   1/1     Running   0             11s   172.16.190.207   workernode    <none>           <none>
managenodes-6b76565695-v8qjs   1/1     Running   0             11s   172.16.190.209   workernode    <none>           <none>
mangenodes-794cb74688-gc8dh    1/1     Running   2 (14m ago)   23h   172.16.216.95    workernode2   <none>           <none>
```

Make the node unscheduleable:
```bash
kubectl cordon workernode2
```
**Note1**

```bash
kubectl get nodes
```
Check the status of workernode2:
```bash
alesb@ControlNode:~$ kubectl get nodes
NAME          STATUS                     ROLES           AGE   VERSION
controlnode   Ready                      control-plane   65d   v1.31.1
workernode    Ready                      <none>          23h   v1.31.1
workernode2   Ready,SchedulingDisabled   <none>          23h   v1.31.1
```

Look for the taints:
```bash
kubectl describe node workernode2 | grep Taints
Taints:             node.kubernetes.io/unschedulable:NoSchedule
```
In spite of the node being cordoned, the pods remained in there:

```bash
NAME                           READY   STATUS    RESTARTS      AGE   IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-4q4fj   1/1     Running   0             26m   172.16.216.99    workernode2   <none>           <none>
managenodes-6b76565695-58q7x   1/1     Running   0             26m   172.16.190.210   workernode    <none>           <none>
managenodes-6b76565695-5vnjj   1/1     Running   0             26m   172.16.216.101   workernode2   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0             26m   172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-djfbp   1/1     Running   0             26m   172.16.216.103   workernode2   <none>           <none>
managenodes-6b76565695-g7n29   1/1     Running   0             26m   172.16.216.100   workernode2   <none>           <none>
managenodes-6b76565695-kw4p4   1/1     Running   0             26m   172.16.190.206   workernode    <none>           <none>
managenodes-6b76565695-nhhhl   1/1     Running   0             26m   172.16.216.102   workernode2   <none>           <none>
managenodes-6b76565695-r7djj   1/1     Running   0             26m   172.16.190.207   workernode    <none>           <none>
managenodes-6b76565695-v8qjs   1/1     Running   0             26m   172.16.190.209   workernode    <none>           <none>
```

But, if we scale-down & scale-up the deployment, all new pods will be placed on workernode:
```bash
kubectl scale deployment managenodes --replicas=1

NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-cckkf   1/1     Running   0          29m   172.16.190.208   workernode   <none>           <none>
```
Scale up to 10 pods:
```bash
kubectl scale deployment managenodes --replicas=10
```
Check the existing pods:
```bash
kubectl get pods -o wide
```


```bash
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-2slns   1/1     Running   0          58s   172.16.190.219   workernode   <none>           <none>
managenodes-6b76565695-6599x   1/1     Running   0          58s   172.16.190.216   workernode   <none>           <none>
managenodes-6b76565695-85hbm   1/1     Running   0          58s   172.16.190.213   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          31m   172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dhq9v   1/1     Running   0          58s   172.16.190.212   workernode   <none>           <none>
managenodes-6b76565695-fnh5m   1/1     Running   0          58s   172.16.190.218   workernode   <none>           <none>
managenodes-6b76565695-qs485   1/1     Running   0          58s   172.16.190.215   workernode   <none>           <none>
managenodes-6b76565695-sj54s   1/1     Running   0          58s   172.16.190.214   workernode   <none>           <none>
managenodes-6b76565695-vnn6b   1/1     Running   0          58s   172.16.190.217   workernode   <none>           <none>
managenodes-6b76565695-z4vlb   1/1     Running   0          58s   172.16.190.211   workernode   <none>           <none>
```

Uncordon the node and check the taints:
```bash
alesb@ControlNode:~$ kubectl uncordon workernode2
node/workernode2 uncordoned
alesb@ControlNode:~$ kubectl describe node workernode2 | grep Taints
Taints:             <none>
```
Will the load-balance occur? The answer is not, except if new pods are coming. That can be forced by doing the following.

By scaling-down & scaling-up again, it will prompt a balance setup again:
```bash
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=1
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=10
deployment.apps/managenodes scaled

kubectl get pods -o wide
NAME                           READY   STATUS        RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-2zkkx   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
managenodes-6b76565695-4f76s   1/1     Running       0          43s     172.16.216.108   workernode2   <none>           <none>
managenodes-6b76565695-7858p   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
managenodes-6b76565695-87lhl   1/1     Running       0          43s     172.16.216.104   workernode2   <none>           <none>
managenodes-6b76565695-8hj6k   1/1     Running       0          43s     172.16.216.106   workernode2   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running       0          34m     172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-n9fhh   1/1     Running       0          43s     172.16.216.109   workernode2   <none>           <none>
managenodes-6b76565695-sz6qh   1/1     Running       0          43s     172.16.216.105   workernode2   <none>           <none>
managenodes-6b76565695-wvtfc   1/1     Running       0          43s     172.16.216.107   workernode2   <none>           <none>
managenodes-6b76565695-zg8lh   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
```

Now, there is a second command already mentioned. 
So, if a node gets drained of pods:
```bash
alesb@ControlNode:~$ kubectl drain workernode2
node/workernode2 cordoned
error: unable to drain node "workernode2" due to error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7, continuing command...
There are pending nodes to be drained:
 workernode2
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7
```

```bash
kubectl drain workernode2 --ignore-daemonsets
node/workernode2 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7
evicting pod ingress-nginx/ingress-nginx-controller-cbcf8bf58-g48m6
evicting pod default/managenodes-6b76565695-4f76s
evicting pod default/managenodes-6b76565695-87lhl
evicting pod default/managenodes-6b76565695-8hj6k
evicting pod default/managenodes-6b76565695-sz6qh
evicting pod default/managenodes-6b76565695-wvtfc
evicting pod default/managenodes-6b76565695-n9fhh
pod/managenodes-6b76565695-87lhl evicted
pod/managenodes-6b76565695-wvtfc evicted
pod/managenodes-6b76565695-8hj6k evicted
pod/managenodes-6b76565695-sz6qh evicted
pod/managenodes-6b76565695-n9fhh evicted
pod/managenodes-6b76565695-4f76s evicted
pod/ingress-nginx-controller-cbcf8bf58-g48m6 evicted
node/workernode2 drained
```

WorkerNode2 has been drained completely:
```bash
kubectl get pods -o wide

NAME                           READY   STATUS    RESTARTS   AGE    IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-22k6m   1/1     Running   0          116s   172.16.190.226   workernode   <none>           <none>
managenodes-6b76565695-29qwr   1/1     Running   0          116s   172.16.190.224   workernode   <none>           <none>
managenodes-6b76565695-2zkkx   1/1     Running   0          6m9s   172.16.190.222   workernode   <none>           <none>
managenodes-6b76565695-4chj9   1/1     Running   0          116s   172.16.190.227   workernode   <none>           <none>
managenodes-6b76565695-7858p   1/1     Running   0          6m9s   172.16.190.221   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          40m    172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dnt55   1/1     Running   0          116s   172.16.190.223   workernode   <none>           <none>
managenodes-6b76565695-qnjsr   1/1     Running   0          116s   172.16.190.228   workernode   <none>           <none>
managenodes-6b76565695-s8rkh   1/1     Running   0          116s   172.16.190.225   workernode   <none>           <none>
managenodes-6b76565695-zg8lh   1/1     Running   0          6m9s   172.16.190.220   workernode   <none>           <none>

```

And important detail for this is that, since the pods were part of a deployment, after the drained happened, the replicas are still matching the target number, 10. And all of them placed on the remaning node, the workernode.

By checking workernode status:
```bash
NAME          STATUS                     ROLES           AGE   VERSION
controlnode   Ready                      control-plane   65d   v1.31.1
workernode    Ready                      <none>          23h   v1.31.1
workernode2   Ready,SchedulingDisabled   <none>          23h   v1.31.1
```
To set it back, it should be uncordoned:
```bash
alesb@ControlNode:~$ kubectl uncordon workernode2
node/workernode2 uncordoned
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-22k6m   1/1     Running   0          3m16s   172.16.190.226   workernode   <none>           <none>
managenodes-6b76565695-29qwr   1/1     Running   0          3m16s   172.16.190.224   workernode   <none>           <none>
managenodes-6b76565695-2zkkx   1/1     Running   0          7m29s   172.16.190.222   workernode   <none>           <none>
managenodes-6b76565695-4chj9   1/1     Running   0          3m16s   172.16.190.227   workernode   <none>           <none>
managenodes-6b76565695-7858p   1/1     Running   0          7m29s   172.16.190.221   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          41m     172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dnt55   1/1     Running   0          3m16s   172.16.190.223   workernode   <none>           <none>
managenodes-6b76565695-qnjsr   1/1     Running   0          3m16s   172.16.190.228   workernode   <none>           <none>
managenodes-6b76565695-s8rkh   1/1     Running   0          3m16s   172.16.190.225   workernode   <none>           <none>
managenodes-6b76565695-zg8lh   1/1     Running   0          7m29s   172.16.190.220   workernode   <none>           <none>
```
But, as it was mentioned before, should be scaled up-down to set pods in the drained node again.
```bash
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=1
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=6
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-5sbz7   1/1     Running   0          42s   172.16.216.113   workernode2   <none>           <none>
managenodes-6b76565695-5tc8z   1/1     Running   0          42s   172.16.216.111   workernode2   <none>           <none>
managenodes-6b76565695-b8c9r   1/1     Running   0          42s   172.16.190.230   workernode    <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          42m   172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-gr69j   1/1     Running   0          43s   172.16.216.110   workernode2   <none>           <none>
managenodes-6b76565695-ms2p4   1/1     Running   0          42s   172.16.216.112   workernode2   <none>           <none>
```

Note1: **Should I have created the deployment at this moment, the result would be the same that after scaling**.