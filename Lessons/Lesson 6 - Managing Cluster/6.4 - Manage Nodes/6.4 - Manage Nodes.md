## Manage Nodes - Node State

There are times on which is necessary to take Pods out of Nodes. Or simple, to do some type of maintenance over the Nodes. The following sections aims to answer that.

--------------------------------------------


With the following commands, nodes will set as unschedulable. Which is great for maintenance.

Mark a node a unschedulable.
```bash
kubectl cordon
``` 

Mark a node as unschedulable and remove all running Pods from it.
```bash
kubectl drain
``` 

Pods that have been started from a DaemonSet will not be removed while using kubectl drain, add --ignore-daemonsets to ignore that. This is because it is supposed to be this way with deamons.

For storage, if pods are using emptyDir Pod volumes, add --delete-emptydir-data to delete data from emptyDir Pod volumes. This are supposed to be ephemeral, anyway.

Get the node back in a schedulable state.
```bash
kubectl uncordon
``` 
---------------------------------------------

### Cordon, Uncordon & Drain in action.

Now, to see it working, lets set a couple a pods running nginx images:

```bash
kubectl create deployment managenodes --image=nginx --replicas=10
```

To start checking this, start by doing:
```bash
kubectl get pods -o wide
``` 

Check the Pod distribution over the nodes. Some of the are running at WorkerNode, whereas other are running at WorkerNode2:
```bash
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS      AGE   IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-58q7x   1/1     Running   0             11s   172.16.190.210   workernode    <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0             11s   172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-kw4p4   1/1     Running   0             11s   172.16.190.206   workernode    <none>           <none>
managenodes-6b76565695-r7djj   1/1     Running   0             11s   172.16.190.207   workernode    <none>           <none>
managenodes-6b76565695-v8qjs   1/1     Running   0             11s   172.16.190.209   workernode    <none>           <none>
mangenodes-794cb74688-gc8dh    1/1     Running   2 (14m ago)   23h   172.16.216.95    workernode2   <none>           <none>
managenodes-6b76565695-nhhhl   1/1     Running   0             11s   172.16.216.102   workernode2   <none>           <none>
managenodes-6b76565695-djfbp   1/1     Running   0             11s   172.16.216.103   workernode2   <none>           <none>
managenodes-6b76565695-g7n29   1/1     Running   0             11s   172.16.216.100   workernode2   <none>           <none>
managenodes-6b76565695-5vnjj   1/1     Running   0             11s   172.16.216.101   workernode2   <none>           <none>
managenodes-6b76565695-4q4fj   1/1     Running   0             11s   172.16.216.99    workernode2   <none>           <none>
```
&nbsp;

Make the node unscheduleable:
```bash
kubectl cordon workernode2
```
**Should I have created the deployment at this moment, the result would be the same that after scaling**.

&nbsp;

```bash
kubectl get nodes
```
Check the status of workernode2:
```bash
alesb@ControlNode:~$ kubectl get nodes
NAME          STATUS                     ROLES           AGE   VERSION
controlnode   Ready                      control-plane   65d   v1.31.1
workernode    Ready                      <none>          23h   v1.31.1
workernode2   Ready,SchedulingDisabled   <none>          23h   v1.31.1
```
&nbsp;

Look for the taints:
```bash
kubectl describe node workernode2 | grep Taints
Taints:             node.kubernetes.io/unschedulable:NoSchedule
```
In spite of the node being cordoned, the pods remained in there:

```bash
NAME                           READY   STATUS    RESTARTS      AGE   IP               NODE          NOMINATED NODE   READINESS GATES

managenodes-6b76565695-58q7x   1/1     Running   0             26m   172.16.190.210   workernode    <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0             26m   172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-kw4p4   1/1     Running   0             26m   172.16.190.206   workernode    <none>           <none>
managenodes-6b76565695-r7djj   1/1     Running   0             26m   172.16.190.207   workernode    <none>           <none>
managenodes-6b76565695-v8qjs   1/1     Running   0             26m   172.16.190.209   workernode    <none>           <none>
managenodes-6b76565695-djfbp   1/1     Running   0             26m   172.16.216.103   workernode2   <none>           <none>
managenodes-6b76565695-g7n29   1/1     Running   0             26m   172.16.216.100   workernode2   <none>           <none>
managenodes-6b76565695-5vnjj   1/1     Running   0             26m   172.16.216.101   workernode2   <none>           <none>
managenodes-6b76565695-4q4fj   1/1     Running   0             26m   172.16.216.99    workernode2   <none>           <none>
managenodes-6b76565695-nhhhl   1/1     Running   0             26m   172.16.216.102   workernode2   <none>           <none>
```
&nbsp;

But, if we scale-down & scale-up the deployment, all new pods will be placed on workernode:
```bash
kubectl scale deployment managenodes --replicas=1

NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-cckkf   1/1     Running   0          29m   172.16.190.208   workernode   <none>           <none>
```
&nbsp;

Scale up to 10 pods:
```bash
kubectl scale deployment managenodes --replicas=10
```
&nbsp;

Check the existing pods:
```bash
kubectl get pods -o wide
```


```bash
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-2slns   1/1     Running   0          58s   172.16.190.219   workernode   <none>           <none>
managenodes-6b76565695-6599x   1/1     Running   0          58s   172.16.190.216   workernode   <none>           <none>
managenodes-6b76565695-85hbm   1/1     Running   0          58s   172.16.190.213   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          31m   172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dhq9v   1/1     Running   0          58s   172.16.190.212   workernode   <none>           <none>
managenodes-6b76565695-fnh5m   1/1     Running   0          58s   172.16.190.218   workernode   <none>           <none>
managenodes-6b76565695-qs485   1/1     Running   0          58s   172.16.190.215   workernode   <none>           <none>
managenodes-6b76565695-sj54s   1/1     Running   0          58s   172.16.190.214   workernode   <none>           <none>
managenodes-6b76565695-vnn6b   1/1     Running   0          58s   172.16.190.217   workernode   <none>           <none>
managenodes-6b76565695-z4vlb   1/1     Running   0          58s   172.16.190.211   workernode   <none>           <none>
```
&nbsp;

Uncordon the node and check the taints:
```bash
alesb@ControlNode:~$ kubectl uncordon workernode2
node/workernode2 uncordoned
alesb@ControlNode:~$ kubectl describe node workernode2 | grep Taints
Taints:             <none>
```
&nbsp;

Will the load-balance occur? The answer is **no**, except if new pods are coming. That can be forced by doing the following.

&nbsp;

By scaling-down & scaling-up again, it will prompt a balance setup again:
```bash
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=1
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=10
deployment.apps/managenodes scaled

kubectl get pods -o wide
NAME                           READY   STATUS        RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-2zkkx   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
managenodes-6b76565695-7858p   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running       0          34m     172.16.190.208   workernode    <none>           <none>
managenodes-6b76565695-zg8lh   0/1     Pending       0          43s     <none>           workernode    <none>           <none>
managenodes-6b76565695-sz6qh   1/1     Running       0          43s     172.16.216.105   workernode2   <none>           <none>
managenodes-6b76565695-wvtfc   1/1     Running       0          43s     172.16.216.107   workernode2   <none>           <none>
managenodes-6b76565695-n9fhh   1/1     Running       0          43s     172.16.216.109   workernode2   <none>           <none>
managenodes-6b76565695-87lhl   1/1     Running       0          43s     172.16.216.104   workernode2   <none>           <none>
managenodes-6b76565695-8hj6k   1/1     Running       0          43s     172.16.216.106   workernode2   <none>           <none>
managenodes-6b76565695-4f76s   1/1     Running       0          43s     172.16.216.108   workernode2   <none>           <none>
```
&nbsp;

Now, there is a second command already mentioned. 
So, if a node gets drained of pods:
```bash
alesb@ControlNode:~$ kubectl drain workernode2
node/workernode2 cordoned
error: unable to drain node "workernode2" due to error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7, continuing command...
There are pending nodes to be drained:
 workernode2
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7
```
&nbsp;

```bash
kubectl drain workernode2 --ignore-daemonsets
node/workernode2 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/calico-node-qwp9t, kube-system/kube-proxy-qlhs7
evicting pod ingress-nginx/ingress-nginx-controller-cbcf8bf58-g48m6
evicting pod default/managenodes-6b76565695-4f76s
evicting pod default/managenodes-6b76565695-87lhl
evicting pod default/managenodes-6b76565695-8hj6k
evicting pod default/managenodes-6b76565695-sz6qh
evicting pod default/managenodes-6b76565695-wvtfc
evicting pod default/managenodes-6b76565695-n9fhh
pod/managenodes-6b76565695-87lhl evicted
pod/managenodes-6b76565695-wvtfc evicted
pod/managenodes-6b76565695-8hj6k evicted
pod/managenodes-6b76565695-sz6qh evicted
pod/managenodes-6b76565695-n9fhh evicted
pod/managenodes-6b76565695-4f76s evicted
pod/ingress-nginx-controller-cbcf8bf58-g48m6 evicted
node/workernode2 drained
```
&nbsp;

WorkerNode2 has been drained completely:
```bash
kubectl get pods -o wide

NAME                           READY   STATUS    RESTARTS   AGE    IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-22k6m   1/1     Running   0          116s   172.16.190.226   workernode   <none>           <none>
managenodes-6b76565695-29qwr   1/1     Running   0          116s   172.16.190.224   workernode   <none>           <none>
managenodes-6b76565695-2zkkx   1/1     Running   0          6m9s   172.16.190.222   workernode   <none>           <none>
managenodes-6b76565695-4chj9   1/1     Running   0          116s   172.16.190.227   workernode   <none>           <none>
managenodes-6b76565695-7858p   1/1     Running   0          6m9s   172.16.190.221   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          40m    172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dnt55   1/1     Running   0          116s   172.16.190.223   workernode   <none>           <none>
managenodes-6b76565695-qnjsr   1/1     Running   0          116s   172.16.190.228   workernode   <none>           <none>
managenodes-6b76565695-s8rkh   1/1     Running   0          116s   172.16.190.225   workernode   <none>           <none>
managenodes-6b76565695-zg8lh   1/1     Running   0          6m9s   172.16.190.220   workernode   <none>           <none>
```
&nbsp;

And important detail for this is that, since the pods were part of a deployment, after the drained happened, the replicas are still matching the target number, 10. And all of them placed on the remaning node, the workernode.

By checking workernode status:
```bash
NAME          STATUS                     ROLES           AGE   VERSION
controlnode   Ready                      control-plane   65d   v1.31.1
workernode    Ready                      <none>          23h   v1.31.1
workernode2   Ready,SchedulingDisabled   <none>          23h   v1.31.1
```
&nbsp;

To set it back, it should be uncordoned:
```bash
alesb@ControlNode:~$ kubectl uncordon workernode2
node/workernode2 uncordoned
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
managenodes-6b76565695-22k6m   1/1     Running   0          3m16s   172.16.190.226   workernode   <none>           <none>
managenodes-6b76565695-29qwr   1/1     Running   0          3m16s   172.16.190.224   workernode   <none>           <none>
managenodes-6b76565695-2zkkx   1/1     Running   0          7m29s   172.16.190.222   workernode   <none>           <none>
managenodes-6b76565695-4chj9   1/1     Running   0          3m16s   172.16.190.227   workernode   <none>           <none>
managenodes-6b76565695-7858p   1/1     Running   0          7m29s   172.16.190.221   workernode   <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          41m     172.16.190.208   workernode   <none>           <none>
managenodes-6b76565695-dnt55   1/1     Running   0          3m16s   172.16.190.223   workernode   <none>           <none>
managenodes-6b76565695-qnjsr   1/1     Running   0          3m16s   172.16.190.228   workernode   <none>           <none>
managenodes-6b76565695-s8rkh   1/1     Running   0          3m16s   172.16.190.225   workernode   <none>           <none>
managenodes-6b76565695-zg8lh   1/1     Running   0          7m29s   172.16.190.220   workernode   <none>           <none>
```
&nbsp;
But, as it was mentioned before, should be scaled up-down to set pods in the drained node again.

&nbsp;

```bash
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=1
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl scale deployment managenodes --replicas=6
deployment.apps/managenodes scaled
alesb@ControlNode:~$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES
managenodes-6b76565695-5sbz7   1/1     Running   0          42s   172.16.216.113   workernode   <none>           <none>
managenodes-6b76565695-5tc8z   1/1     Running   0          42s   172.16.216.111   workernode   <none>           <none>
managenodes-6b76565695-b8c9r   1/1     Running   0          42s   172.16.190.230   workernode    <none>           <none>
managenodes-6b76565695-cckkf   1/1     Running   0          42m   172.16.190.208   workernode2    <none>           <none>
managenodes-6b76565695-gr69j   1/1     Running   0          43s   172.16.216.110   workernode2   <none>           <none>
managenodes-6b76565695-ms2p4   1/1     Running   0          42s   172.16.216.112   workernode2   <none>           <none>
```

--------------------------------------------------

Note1: **Should I have created the deployment at this moment, the result would be the same that after scaling**.