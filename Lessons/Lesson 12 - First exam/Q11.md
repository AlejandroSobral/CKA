## Question 11: Configuring Taints and Tolerations
Configure node worker2 such that it will only allow Pods to run that have been configured with the setting type:db
After verifying this works, remove the node restriction to return to normal operation.

------------------------------------------------------------------------------------------

## Demo

Checking the [Docs](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)

```
"Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods...
Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints."
```


1) Taint node Worker2
2) Create two deployments. Set Pod toleration on one only.
3) Check status
4) Revert taint of the node.




Taint the node, using the help:
```bash
kubectl describe node talos-24t-iqs | grep Taints
Taints:             <none>

kubectl taint -h

# Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
# If a taint with that key and effect already exists, its value is replaced as specified
kubectl taint nodes foo dedicated=special-user:NoSchedule

kubectl taint nodes talos-24t-iqs type=db:NoSchedule
node/talos-24t-iqs tainted

Taints:             type=db:NoSchedule

 kubectl get pods
No resources found in default namespace.
```

&nbsp;

Create the deployment, with and without tolerations:
```bash
kubectl create deployment no-tolerations --image=nginx --port=80 --replicas=2
deployment.apps/no-tolerations created

kubectl create deployment tolerations --image=nginx --port=80 --replicas=2 --dry-run=client -o yaml
kubectl apply -f tol-dep.yaml
deployment.apps/tolerations created
```

&nbsp;

Add the taint to tol-dep.yaml
```YAML
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: tolerations
  name: tolerations
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tolerations
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: tolerations
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources: {}
      tolerations:
      - key: "type"
        operator: "Equal"
        value: "db"
status: {}
```

&nbsp;

Add more replicas to make the effect explicit.
```bash
kubectl scale deployment no-tolerations --replicas=5
deployment.apps/no-tolerations scaled

kubectl scale deployment tolerations --replicas=5
deployment.apps/tolerations scaled
```

&nbsp;


As expected, all Pods with no tolerations runs at the no-tainted Node. Whereas the other, runs in both.
```bash
NAME                              READY   STATUS    RESTARTS   AGE     IP           NODE            NOMINATED NODE   READINESS GATES
no-tolerations-5bfd68cd96-h554l   1/1     Running   0          3m17s   10.244.1.3   talos-svg-nsf   <none>           <none>
no-tolerations-5bfd68cd96-hgspf   1/1     Running   0          27s     10.244.1.5   talos-svg-nsf   <none>           <none>
no-tolerations-5bfd68cd96-sl9wf   1/1     Running   0          27s     10.244.1.6   talos-svg-nsf   <none>           <none>
no-tolerations-5bfd68cd96-t2fhc   1/1     Running   0          3m17s   10.244.1.2   talos-svg-nsf   <none>           <none>
no-tolerations-5bfd68cd96-v66l9   1/1     Running   0          27s     10.244.1.7   talos-svg-nsf   <none>           <none>
tolerations-6bc7c7c64f-6v7zp      1/1     Running   0          21s     10.244.2.3   talos-24t-iqs   <none>           <none>
tolerations-6bc7c7c64f-9mmpc      1/1     Running   0          21s     10.244.1.8   talos-svg-nsf   <none>           <none>
tolerations-6bc7c7c64f-b7nbf      1/1     Running   0          21s     10.244.2.4   talos-24t-iqs   <none>           <none>
tolerations-6bc7c7c64f-d26cs      1/1     Running   0          106s    10.244.2.2   talos-24t-iqs   <none>           <none>
tolerations-6bc7c7c64f-wvgjb      1/1     Running   0          106s    10.244.1.4   talos-svg-nsf   <none>           <none>
```

&nbsp;

And that's it!

To finish, delete the taint on the node.
```bash
kubectl taint nodes talos-24t-iqs type=db:NoSchedule-
node/talos-24t-iqs untainted
```

You can now, try to scale down & up, and check the Pod distribution.